---
title: "Cleaning Data"
output:
  pdf_document: default
  html_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
default_width <- 80
options(width = default_width)
```

## Cleaning Data In R

GitHub Repository with this demonstration: [https://github.com/unmrds/R-data-cleaning](https://github.com/unmrds/R-data-cleaning)

Zipfile Download: [https://github.com/unmrds/R-data-cleaning/archive/master.zip](https://github.com/unmrds/R-data-cleaning/archive/master.zip)

PDF Version of this Notebook: [https://github.com/unmrds/R-data-cleaning/raw/master/code/cleaning_data.pdf](https://github.com/unmrds/R-data-cleaning/raw/master/code/cleaning_data.pdf)

Shared [RStudioCloud](https://rstudio.cloud/) version that you can copy into your account to work with: [https://rstudio.cloud/project/3081275](https://rstudio.cloud/project/3081275)

Check out the **Preflight Check** to see if you have the needed R libraries installed run the `package.check.R` script in the top directory of this R project. This script will check to see if the packages are installed, and if they are not will install them. The script finishes with a listing of the currently installed packages so you can verify that the packages we need are installed. Check out the [README.md](https://github.com/unmrds/R-data-cleaning/blob/master/README.md) file in the [workshop repository](https://github.com/unmrds/R-data-cleaning) for full setup instructions. 

When planning a data analysis the first step, and often most time consuming, is the acquisition and processing of the data into a form that can be used in the analytic procedures you intend to use. Today we are going to focus on a sequence of steps that generally follow the workflow that you will find yourself going through when bringing data into R to perform an analysis. 

![Portion of *R for Data Science*^[Hadley Wickham & Garrett Grolemund. 2017. *R for Data Science*. O’Reilly. [https://r4ds.had.co.nz](https://r4ds.had.co.nz) ] workflow](images/tidy_workflow.png)

### Deal with issues that may come up when importing data files

1. Identify and correct structural issues in the source data that prevent clean import into R data structures
2. Check and handle data type errors
3. Check and handle missing data values

### Tuning up the structure of the data to facilitate analysis

4. Split up fields that contain multiple values in a single field
5. Check for anomalous values and otherwise explore the data to become familiar with its content and structure. 

*Beyond what we will cover today* - continued structural changes and the rest of the exploration, analysis, and communication process. 

### Data for today's demonstration

The data for this demonstration are based upon the [idigbio_rodents.csv](https://figshare.com/articles/idigbio_rodents_csv/5535724) dataset. The data are described as follows in the repository where they are shared:

> The idigbio_rodents.csv dataset is just over 10k records and contains data from natural history collections specimen records representing 5 genera from 4 major US collections, limited to US records. All records are from the Order Rodentia. All the data are mapped to the biodiversity data standard Darwin Core (http://rs.tdwg.org/dwc/terms/).

Collins, Matthew; Paul, Deborah (2017): idigbio_rodents.csv. figshare. Dataset. [https://doi.org/10.6084/m9.figshare.5535724.v1](https://doi.org/10.6084/m9.figshare.5535724.v1) 

The original data have been modified for use in this demonstration by:

1. Generating new data columns (`latDMS` and `lonDMS`) for latitude and longitude that have sample coordinates presented in Degrees-Minutes-Seconds instead of the originally provided decimal degrees.
2. Generating a column of mixed numeric and text values - `textLatDD`. 

The `breaking_data.Rmd` R notebook file programmatically performs the above changes and automatically generated the `learning.csv`. 

This is the `../data/learning.csv` file. These newly created columns in addition to some of the originally provided ones will be used to demonstrate a variety of data cleaning steps in R.

An additional file was developed that only includes the first 10 rows of the file (including headers) but introduces a structural error. This file is the `../data/learning_struct.csv` file. 

### R libraries used in the demonstration

For this demonstration a combination of R packages that are part of the [*Tidyverse*](https://www.tidyverse.org) and additional specialized data evaluation packages will be used. The tidyverse collection of packages provide (as described on the project's homepage):

> an opinionated collection of R packages designed for data science. All packages share an underlying design philosophy, grammar, and data structures. 

In addition to `tidyverse`, this workshop also uses the `mice`, `VIM`, and `assertr` packages. 

There are currently over 14,000 R [packages](https://cran.r-project.org/web/packages/index.html) in the [Comprehensive R Archive Network (CRAN)](https://cran.r-project.org/index.html). While the tidyverse packages provide a useful degree of consistency and internal interoperability it is strongly encouraged to examine the broad collection of R packages when working on a particular analysis problem. 

If you need to install any of the needed packages in your environment you can execute the `install.packages("<package name>")` command in the R console. The `package.check.R` script mentioned above will both check to see if the packages are installed, and if needed, install them. 

now onto the data  processing ...

## 1. Identify and correct structural issues in the source data that prevent clean import into R data structures

R can import a wide variety of *rectangular* data structures: comma-delimited, tab-delimited, excel spreadsheets, fixed-width among the many options. If there are errors in the structure of these files, R import commands may not be able to parse the lines in the data file preventing import. In these cases the returned error messages *may* provide some clues to where the errors may be found.  

**One strategy for identifying potential structural issues in the source file is to try to import the dataset and review any errors that are returned**

Let's try it first with a small file ...

```{r}
########## Working with a CSV file with structural problems ###################

library(tidyverse)

# Import the source CSV file that contains a structural flaw
# ? what does that path string in the read_csf function mean?
rawDataStruct <- read_csv("../data/learning_struct.csv", 
                    progress = FALSE)

# Display the column definitions for the imported dataset
spec(rawDataStruct)

# Report the problems that were encountered when the data were imported.
problems(rawDataStruct)

# Display the imported table
rawDataStruct
```

The output of the `read_csv` command and of the `problems` function indicate that there was a problem of some sort around row 7 during the import of the CSV:

  1 parsing failure.
  row col   expected     actual                          file
  7  -- 24 columns 26 columns '../data/learning_struct.csv'

Let's take a look at the source data file (in your favorite text editor or within the RStudio interface) and see if we can find the problem...

**What did we find?**

### After the structural problem has been resolved load the full dataset for use in the rest of the workshop



```{r}
########## Load the full CSV file without the structural error ################

library(tidyverse)

# Import the source CSV file that does not contain the structural problem highlighted above
rawData <- read_csv("../data/learning.csv", 
                    progress = FALSE)

# Display the column definitions for the imported dataset
spec(rawData)

# Report the problems that were encountered when the data were imported.
problems(rawData)

# Display the first few rows of the imported table
head(rawData, n=20)

```

Some questions:

1. How do the data types for the columns from this import process differ from those in the previous subset (at least before we fixed it)? Why do you think this is the case?
2. Where there any errors identified during the import? If no, does this mean that there are no potential problems or issues with the imported data? **Let's take a look**
3. How would you explain the values in the `eventDate` column when compared to the `year`, `month`, and `day` columns? *Hint - what do we know about timezones in this dataset?*
4. What were the different ways in which missing data values were handled?

## 2. Checking and handling data type errors

Depending on the types of data that are encountered in each column of the imported dataset different R import functions will automatically "type" the column (or in R terminology set the "mode" of the column) based on some sample of rows from the source file. In the case of `readr`, the data reading package used by tidyverse, the first 1000 lines of data will be read to determine the data type that should be used for each column. The core R data types are:

* character
* numeric (real or decimal)
* integer
* logical
* complex

These core data types can then be used as the foundation for more complex data types such as *dates*, *times* and *datetimes*. 

The core data structures that can be used to organize collections of these data types include:

* vector - a sequence of data values of the same type
* list - a sequence of data values of the same or different types, and structures
* matrix - a vector for which one or more *dimensions* are defined
* data frame (and the "tibble" in the tidyverse) - a structured collection of vectors of the same length
* factors - a factor vector is a set of integer values that are associated with a collection of categorical character values

Let's focus on the `catalogNumber` and `textLatDD` columns in our sample datasets. 

```{r}
########## Handling data type errors on import ################################

## Take a look at the types and content of a couple of columns
spec(rawData)
rawData %>%
  select(catalogNumber,textLatDD) %>%       # let's just look at the "catalogNumber" and "textlatDD" columns
  slice_head(n=20)                          # let's just look at the first 20 rows
```


```{r}
## Test the creation of a numLatDD column as a numeric column and
## see what rows were converted to NA
rawData %>%
  mutate(numLatDD = as.numeric(rawData$textLatDD)) %>%      # convert all of the values to numbers if possible
  filter(is.na(numLatDD)) %>%                               # select all of the rows in the new numLat column has a NA value
  select(textLatDD, numLatDD) %>%                           # include only the "textLatDD" and "numLatDD" columns
  print() %>%                                               # print the resulting rows and columns
  group_by(textLatDD) %>%                                   # aggregate rows by the content of the "textLatDD" column
  summarize(count = n())                                    # count up the number of values in each group 

```

```{r}
## create a numeric column based on the previously tested conversion of 
## the textLatDD column
rawData$numLatDD <- as.numeric(rawData$textLatDD)
head(rawData, n = 20)
```

We can also accomplish a similar outcome by specifying the column type that should be created as part of the import process. 

```{r}
## Specify the column data type when importing 
rawData2 <- read_csv("../data/learning.csv", 
                     col_types = cols(
                        textLatDD = col_double()
                        ),
                     progress = FALSE)

# Display the column definitions for the imported dataset
spec(rawData2)

# Report the problems that were encountered when the data were imported.
problems(rawData2)

# Display the imported table
head(rawData2, n = 20)
```

```{r}
## Convert the catalogNumberTxt column to a character column and see what 
## the result is
rawData %>%
  mutate(catalogNumberTxt = as.character(catalogNumber)) %>%
  filter(is.na(catalogNumberTxt))

```
What do you think would be more forgiving (i.e. less error prone), conversion from character to numeric values or the reverse?


## 3. Check and handle missing values

It is important to understand the potential impact that missing data will have on your analysis. As we've already seen the import process may automatically produce missing data values in your analysis dataframe (or *tibble* in the context of tidyverse based processes). Some functions enable you to efficiently visualize the patterns of missing values in your dataset - allowing for the analysis of large datasets that otherwise would not be feasible to review manually. 

```{r}
########## Check and handle missing values ####################################

## Manually by printing out sum (FALSE = 0, TRUE = 1) of is.na test results
# remember that the paste() function just lets you concatenate pieces of text (or values that can be coerced into text) into a single text string
paste("decimalLatitude: number of NA values", 
      sum(is.na(rawData$decimalLatitude)), 
      sep = " ")
paste("decimalLongitude: number of NA values", 
      sum(is.na(rawData$decimalLongitude)), 
      sep = " ")
paste("weight: number of NA values", 
      sum(is.na(rawData$weight)), 
      sep = " ")
paste("length: number of NA values", 
      sum(is.na(rawData$length)), 
      sep = " ")
paste("sex: number of NA values", 
      sum(is.na(rawData$sex)), 
      sep = " ")
paste("latDMS: number of NA values", 
      sum(is.na(rawData$latDMS)), 
      sep = " ")
paste("lonDMS: number of NA values", 
      sum(is.na(rawData$lonDMS)), 
      sep = " ")
paste("textLatDD: number of NA values", 
      sum(is.na(rawData$textLatDD)), 
      sep = " ")
paste("numLatDD: number of NA values", 
      sum(is.na(rawData$numLatDD)), 
      sep = " ")

```

In this analysis we will be using the `md.pattern` function that is part of the `mice` package, and the `aggr` function which is part of the `VIM` package. If you haven't already installed the `mice` and `VIM` packages you can do so by executing the `install.packages("mice")` and `install.packages("VIM")` commands.  

```{r set-options, echo=FALSE, cache=FALSE}
## View the combinations of NA values across multiple columns with the 
## md.pattern function from the mice package

library(mice)
options(width = 120)  # set the display width so the text doesn't wrap
par(cex=0.75)   #reduce the default font size so the numbers don't overlap

rawData %>%
  select(decimalLatitude, 
         decimalLongitude, 
         weight, 
         length, 
         sex, 
         latDMS, 
         lonDMS, 
         textLatDD, 
         numLatDD) %>%
  md.pattern(rotate.names = TRUE)

options(width = default_width)
```

Another method of viewing similar information

```{r}
## View the combinations of NA values across multiple columns with the 
## aggr function from the VIM package
library(VIM)

rawData %>%
  select(decimalLatitude, 
         decimalLongitude, 
         weight, 
         length, 
         sex, 
         latDMS, 
         lonDMS, 
         textLatDD, 
         numLatDD) %>%
  rename(latDD = decimalLatitude, lonDD = decimalLongitude) %>%
  aggr(numbers=TRUE)
```


## 4. Multi-value columns

A core principle of having well structured data that is read for analysis is that:

In the context of "Tidy" data that underlie the tools developed as part of the tidyverse package^[Hadley Wickham & Garrett Grolemund. 2017. *R for Data Science*. O’Reilly. - section on Tidy Data [https://r4ds.had.co.nz/tidy-data.html](https://r4ds.had.co.nz/tidy-data.html)

> There are three interrelated rules which make a dataset tidy:
>
> 1. Each variable must have its own column.
> 2. Each observation must have its own row.
> 3. **Each value must have its own cell.**

This issue also relates to the idea of *atomicity* in Codd's definition of *First Normal Form* when *normalizing* a relational database^[https://en.wikipedia.org/wiki/First_normal_form]. While we're not going to get into relational data modeling in R here, well structured data allow for the use of relational data models in your analysis independent of a separate database server. 

In this example we are going to focus on three columns: `recordedBy`, `latDMS`, and `lonDMS`. 

```{r}
########## Handling multi-value columns #######################################

## Take a look at the recordedBy, latDMS, and lonDMS columns
rawData %>%
  select(recordedBy, latDMS, lonDMS)
```

Breaking apart the `recordedBy` column using the `str_match` function from the `stringr` package. This uses *regular expressions* for defining the text patterns that should be found and processed in the process of breaking the column apart into new columns. Regular expressions are an art to themselves and there are many resources for learning their effective use - ranging from one-page cheat-sheets to full length books. Some great resources include:

* Jeffrey E.F. Friedl (2006) *Mastering Regular Expressions*. 3rd Ed. O'Reilly. [http://shop.oreilly.com/product/9780596528126.do](http://shop.oreilly.com/product/9780596528126.do) and for UNM affiliates through our Safari Online Learning subscription: [https://learning.oreilly.com/library/view/mastering-regular-expressions/0596528124/](https://learning.oreilly.com/library/view/mastering-regular-expressions/0596528124/) - this is a comprehensive, in-depth treatment of regular expressions from the most simple to most complex - including disucssions of implementations of regular expression support in different programming languages. 
* Steven Levithan, Jan Goyvaerts (2012). *Regular Expressions Cookbook*. 2nd Ed. O'Reilly. [http://shop.oreilly.com/product/0636920023630.do](http://shop.oreilly.com/product/0636920023630.do) and for UNM affiliates [https://learning.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/](https://learning.oreilly.com/library/view/regular-expressions-cookbook/9781449327453/) - a good set of regular expression common problems, and their solutions in multiple language implementations. 
* Ian Kopacka (2016). *Basic Regular Expressions in R - Cheat Sheet*. Online resource: https://github.com/rstudio/cheatsheets/raw/master/regex.pdf](https://github.com/rstudio/cheatsheets/raw/master/regex.pdf)


The following figure describes the structure of the regular expressions used in the sample code:

![Description of the regular expression used to extract targeted substrings from the combined "recordedBy" field](images/regex.png)


```{r}
## Define and use some R regular expressions for extracting text from the 
## recordedBy column
collectorExtract <- "^collector\\(s\\):\\s(.*;|.*$)"
preparatorExtract <- "preparator\\(s\\):\\s(.*;|.*$)"

collector_string <- str_match(rawData$recordedBy, collectorExtract)
preparator_string <- str_match(rawData$recordedBy, preparatorExtract)

print(head(collector_string))
print(head(preparator_string))

rawData$collectors <- collector_string[,2]
rawData$preparators <- preparator_string[,2]

# check the first ten rows to see what the output looks like
head(rawData, n=10) %>%
  select(recordedBy, collectors, preparators)
```

*What would the next logical step in the process be for cleaning up the `recordedBy` or newly generated columns?*

Breaking apart the `latDMS` and `lonDMS` columns into their constituent parts

```{r}
## Define a regular expression and use it to extract pieces from a DMS string
dmsExtract <- "\\s*(-*[:digit:]+)°\\s*([:digit:]+)\\'\\s*([:digit:]+\\.*[:digit:]*)"

latSubstrings <- str_match(rawData$latDMS, dmsExtract)
print(head(latSubstrings))

rawData$latD <- as.numeric(latSubstrings[,2])
rawData$latM <- as.numeric(latSubstrings[,3])
rawData$latS <- as.numeric(latSubstrings[,4])

lonSubstrings <- str_match(rawData$lonDMS, dmsExtract)
print(head(lonSubstrings))

rawData$lonD <- as.numeric(lonSubstrings[,2])
rawData$lonM <- as.numeric(lonSubstrings[,3])
rawData$lonS <- as.numeric(lonSubstrings[,4])


head(rawData, n=10) %>% 
  select(latDMS, latD, latM, latS,
         lonDMS, lonD, lonM, lonS)
```

## 5. Check value ranges and explore data

As part of the examiniation of these columns we will use the `assertr`. If you need to install the `assertr` package in your enviornment you can execute the `install.packages("assertr")` command. 

Checking the `weight` and `length` columns. 

```{r}
########## Check value ranges and explore data ################################

## assert GitHub repository with instructions and examples: 
##      https://github.com/ropensci/assertr
library(assertr)

## you can just run this if you want execution of your workflow to stop if 
## any of your tests fail

# rawData %>% 
#   chain_start %>%
#   assert(within_bounds(1,Inf), weight) %>% # assert checks individual values
#   assert(within_bounds(1,Inf), length) %>%
#   insist(within_n_sds(3), weight) %>% # insist checks against calculated vals
#   insist(within_n_sds(3), length) %>%
#   chain_end

## you can run this if you want to prevent the errors that are generated
## by your tests from halting execution of your workflow
tryCatch({rawData %>%
  slice_sample(n = 1000) %>%                          # limit the example to 1000 randomly selected rows
  chain_start %>%
  assert(within_bounds(1,Inf), weight) %>%            # assert checks individual values
  assert(within_bounds(1,Inf), length) %>%
  insist(within_n_sds(3), weight) %>%                 # insist checks against calculated vals
  insist(within_n_sds(3), length) %>%
  chain_end
}, warning = function(w) {
    paste("A warning was generated: ", w, sep = "")
}, error = function(e) {
    print(e)
}, finally = {
    print("this is the end of the validation check ...")
  }
)
```

```{r}
## Visual assessment of your data 
library(ggplot2)

ggplot(rawData, aes(x=weight)) +
  geom_histogram(binwidth = 5)
ggplot(rawData, aes(x=length)) +
  geom_histogram(binwidth = 5)
  
```


## 6. Bringing it all together to produce our analysis dataset

```{r}
###############################################################################
########## Bringing it all together into a single series of commands ##########

# import the data with explicit column definitions for the textLatDD and catalogNumber columns
analysisData <- read_csv("../data/learning.csv", 
                     col_types = cols(
                        textLatDD = col_double(), 
                        catalogNumber = col_character()
                        ),
                     progress = FALSE)

# split up the recordedBy column
collectorExtract <- "^collector\\(s\\):\\s(.*;|.*$)"
preparatorExtract <- "preparator\\(s\\):\\s(.*;|.*$)"

collector_string <- str_match(rawData$recordedBy, collectorExtract)
preparator_string <- str_match(rawData$recordedBy, preparatorExtract)

rawData$collectors <- collector_string[,2]
rawData$preparators <- preparator_string[,2]

# split up the latDMS and lonDMS columns
dmsExtract <- "\\s*(-*[:digit:]+)°\\s*([:digit:]+)\\'\\s*([:digit:]+\\.*[:digit:]*)"

latSubstrings <- str_match(rawData$latDMS, dmsExtract)

rawData$latD <- as.numeric(latSubstrings[,2])
rawData$latM <- as.numeric(latSubstrings[,3])
rawData$latS <- as.numeric(latSubstrings[,4])

lonSubstrings <- str_match(rawData$lonDMS, dmsExtract)

rawData$lonD <- as.numeric(lonSubstrings[,2])
rawData$lonM <- as.numeric(lonSubstrings[,3])
rawData$lonS <- as.numeric(lonSubstrings[,4])

glimpse(analysisData)

```

Export the code from the workshop both as a documented and undocumented R script

```{r}
#library(knitr)
#purl("cleaning_data.Rmd", "cleaning_data_nodocs.R", documentation = 0)
```

